# [cite_start]Aravind Remanan Kumary Asha, Msc [cite: 1]

- **Contact:** aremanan.career@gmail.com | Stuttgart, Baden-Württemberg | [cite_start]+49 15751666770 [cite: 2]
- [cite_start]**Links:** @linkedin @Github [cite: 2]

## Summary
[cite_start]Certified Scrum Master with expertise in transforming complex datasets into actionable insights using advanced analytics, statistics, and machine learning[cite: 3]. [cite_start]I lead end-to-end data projects, from data modeling and pipeline development to model deployment, leveraging Agile methodologies to drive data-driven decisions and solve real-world challenges across diverse domains[cite: 4].

## Experience

### [cite_start]Scientific Activity - DLR (Deutsches Zentrum für Luft- und Raumfahrt e.V.) [cite: 17, 19]
**Stuttgart, DE | [cite_start]06/2024 - present** [cite: 20]
- [cite_start]Automated end-to-end data workflows using FastAPI for API development and Docker for containerized deployments, streamlining data preprocessing, storage, and visualization for Alkaline Water Electrolysis (AWE) test bench[cite: 21].
- [cite_start]Developed Human-Machine Interface (HMI) applications and integrated time series models with Python, improving predictive insights and anomaly detection by 73% for production data[cite: 22].
- [cite_start]Engineered data management pipelines for material performance and quality assurance for real-time streaming and PostgreSQL for structured storage, adhering to industry-standard data validation[cite: 23].
- [cite_start]Implemented real-time data streaming from industrial robots using OPC UA and orchestrated ETL processes, ensuring data integrity and scalability[cite: 24].
- [cite_start]Utilized Great Expectations/pydantic for data validation meeting industry benchmarks for reliable and scalable data pipelines[cite: 25].

### [cite_start]Master Thesis/ Wissenschaftliche Hilfskraft - DLR [cite: 26]
**Stuttgart, DE | [cite_start]08/2023 - 04/2024** [cite: 27, 20]
- [cite_start]Conducted research on heterogeneous data-driven optimization and automation for Robotic Screw Extrusion Additive Manufacturing (SEAM) using KUKA robots and RoboDK, integrating Industry 4.0 and IIoT frameworks to enhance operational efficiency[cite: 28].
- [cite_start]Designed and optimized graph database schemas in Shepard, incorporating semantics and ontologies to ensure compliance with FAIR data principles[cite: 29].
- [cite_start]Developed real-time data streaming pipelines for industrial robots using OPC UA and orchestrated ETL processes, enabling robust data modeling for graph databases[cite: 30].
- [cite_start]Performed advanced statistical analysis with SciPy and Pandas to integrate digital twin models into the data lifecycle, improving precision in additive manufacturing processes[cite: 31].
- [cite_start]Leveraged FastAPI for API development and Docker for containerized environments, ensuring scalable and reproducible data workflows[cite: 32].
- [cite_start]Applied Agile practices to manage thesis milestones, ensuring timely delivery and alignment with research objectives[cite: 33].

### [cite_start]Hochvolt Lademanagement - Praktikant - Porsche Engineering [cite: 35, 36]
**Stuttgart, DE | [cite_start]05/2023 - 07/2023** [cite: 37, 38]
- [cite_start]Client: Audi / Porsche [cite: 36]
- [cite_start]Developed and optimized battery testing code to improve code deployment time by 20%[cite: 39].
- [cite_start]Supported CI/CD processes to integrate and take part in deployment process[cite: 40].
- [cite_start]Provided project support and maintained documentation in Confluence, ensuring effective collaboration[cite: 41].

### [cite_start]Data Science Intern (Advanced driver-assistance system) - Cariad (Volkswagen group) [cite: 42, 45]
**Stuttgart, DE | [cite_start]08/2022 - 02/2023** [cite: 43, 46]
- [cite_start]Client: Audi / Porsche [cite: 45]
- [cite_start]Enhanced Adaptive Cruise Control efficiency by 20% through a Python Pandas-based KPI analysis framework on time-series drive data from CAN[cite: 47].
- [cite_start]Automated reporting processes using Jupyter notebooks, reducing reporting time by 90% and boosting workflow efficiency[cite: 48].
- [cite_start]Developed cloud-based data pipelines and analytics solutions, collaborating with Agile teams to improve project efficiency by 25%[cite: 49].

### [cite_start]Sr. Systems Engineer (Delivery) - Infosys Ltd [cite: 51, 52]
**Hyderabad, IN | [cite_start]03/2016 - 04/2021** [cite: 53, 54]
- [cite_start]Client: Insulet corporation [cite: 52]
- [cite_start]Led IoT data forecasting and analytics team as Scrum Master, transforming millions of transactions into actionable metrics using Python and SQL[cite: 55].
- [cite_start]Automated data extraction and transformation, boosting operational efficiency by 70%[cite: 56].
- [cite_start]Developed strategic dashboards and automated reports with Tableau and Python, providing real-time insights[cite: 57].
- [cite_start]Extracted and managed data from Oracle databases using SQL and PL/SQL[cite: 58].
- [cite_start]Collaborated with business partners to create innovative, data-driven solutions aligned with strategic goals[cite: 59].
- [cite_start]Communicated analytics insights to stakeholders, emphasizing data-driven decision-making[cite: 60].

## Education

- [cite_start]**University of Koblenz Landau** - MSc Web and Data Science, Koblenz, DE [cite: 7, 12]
- [cite_start]**Lappeenranta University of Technology** - ERASMUS Plus Exchange Program, Lappeenranta, FL [cite: 6, 11]
- [cite_start]**Amrita Vishwa Vidyapeetham** - Bachelor of Computer Application, Mysore, IN [cite: 8, 9, 13]

## Publications

- [cite_start]A Digital Process Chain-Integrated Data Acquisition Module for Robotic Screw Extrusion Additive Manufacturing - Anwenderforum Additive Produktionstechnologie 2025 [cite: 14]
- Optimizing Heterogenous Data Management for Robotic Screw Extrusion Additive Manufacturing. [cite_start]Thesis (Master's) 2024 [cite: 15]

## Projects

### [cite_start]Data Quality Management in Cloud for Time Series Data [cite: 62]
**Koblenz, DE | [cite_start]2023** [cite: 66]
- [cite_start]Design and development of data quality framework for time series data[cite: 63].
- [cite_start]Full documentation of the project including maintenance documents[cite: 64].
- [cite_start]**Tech stack:** Databricks [cite: 62]

## Skills

- [cite_start]**Languages:** Python (Pandas, NumPy, Matplotlib, Scikit-Learn, TensorFlow, PyTorch), Java, SQL, C++ (Embedded for short term) [cite: 67]
- [cite_start]**Development Tools:** Jupyter Notebook, Anaconda, Visual Studio Code [cite: 68]
- [cite_start]**Visualization/Big Data Tools:** Tableau, Streamlit, Taipy [cite: 69]
- [cite_start]**Databases:** PostgreSQL, MySQL, MS SQL Server, MongoDB, InfluxDB, Shepard DB, SQLite, Neo4j [cite: 70]
- [cite_start]**Cloud & DevOps:** Databricks, Docker, Kubernetes, Git (Bitbucket), CI/CD (GitLab, Jenkins) [cite: 71]
- [cite_start]**IIoT & Automation:** MQTT, OPC UA, RoboDK, KUKA RSI [cite: 73]
- [cite_start]**Project Management Tools:** JIRA, Miro, Confluence, Trello, Microsoft Office Suite, Slack, Azure DevOps [cite: 72]
- [cite_start]**Other Skills:** Data Engineering, Digital Twin, Industry 4.0, Agile Methodologies (Infosys Certified Scrum Master) [cite: 74]

## Languages

- [cite_start]Fluent English [cite: 75]
- [cite_start]B1 Deutsch [cite: 75]